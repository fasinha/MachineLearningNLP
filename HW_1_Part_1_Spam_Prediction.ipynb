{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW-1 Part 1. Spam Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p7Z8eeN5IW9q"
      },
      "source": [
        "# Part 1.\n",
        "\n",
        "The deadline for Part 1 is **1:30 pm Feb 6, 2020**.   \n",
        "You should submit a `.ipynb` file with your solutions to NYU Classes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this part we will preprocess SMS Spam Collection Dataset and train a bag-of-words classifier (logistic regression) for spam detection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dZd0LJzbISPd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "First, we download the SMS Spam Collection Dataset. The dataset is taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loaded to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR) so that everyone can access it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FeyuutEKNnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a53bd844-427f-49af-dc9d-f66dcbd1c121"
      },
      "source": [
        "!pip install wget\n",
        "import wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=aceaf363b1b32d3ffd1a8a277db170167e2bafa84a3062987674de76ddc4b46e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PvGErs2oHkWU",
        "outputId": "d3add769-5804-470f-92ae-0ae8a5fb01d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-03 21:18:38--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.101, 108.177.125.100, 108.177.125.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/aie1bsnedlj43tn9bb8la3928q9gtbvn/1580760000000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-02-03 21:18:39--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/aie1bsnedlj43tn9bb8la3928q9gtbvn/1580760000000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
            "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 74.125.23.132, 2404:6800:4008:c02::84\n",
            "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|74.125.23.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/csv]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-02-03 21:18:40 (147 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcHV1lUwtH-n",
        "outputId": "784cd3b7-10b0-41ca-cd7b-e233d120f67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spam.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eXVQCF-ovo4G"
      },
      "source": [
        "There are two columns: `v1` -- spam or ham indicator, `v2` -- text of the message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGQDVwzlKNne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6b780060-d870-4b40-d702-3576c688a016"
      },
      "source": [
        "!pip install pandas"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiKE89v0zMiY",
        "outputId": "e3e6cf8c-fa92-40d6-a9ab-25c1efc6bd1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
        "# 1 - spam, 0 - ham\n",
        "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   v1                                                 v2\n",
              "0   0  Go until jurong point, crazy.. Available only ...\n",
              "1   0                      Ok lar... Joking wif u oni...\n",
              "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   0  U dun say so early hor... U c already then say...\n",
              "4   0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nXQhTzrCv-Nk"
      },
      "source": [
        "Your task is to split the data to train/dev/test. Make sure that each row appears only in one of the splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ga5Qydpw-gdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "6ff9d907-2fde-44ee-9508-d65e71cd6b74"
      },
      "source": [
        "# 0.15 for val, 0.15 for test, 0.7 for train\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "# val = df.iloc[0:val_size]\n",
        "# test = df.iloc[val_size:val_size+test_size]\n",
        "# train = df.iloc[val_size+test_size:]\n",
        "\n",
        "train_texts, train_labels = None, None\n",
        "val_texts, val_labels     = None, None\n",
        "test_texts, test_labels   = None, None\n",
        "print(df)\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "print(df)\n",
        "val_texts = df.iloc[0:val_size].v2\n",
        "val_labels = df.iloc[0:val_size].v1\n",
        "#print(val_texts)\n",
        "test_texts = df.iloc[val_size:val_size+test_size].v2\n",
        "test_labels = df.iloc[val_size:val_size+test_size].v1\n",
        "\n",
        "train_texts = df.iloc[val_size+test_size:].v2\n",
        "train_labels = df.iloc[val_size+test_size:].v1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      v1                                                 v2\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      0  U dun say so early hor... U c already then say...\n",
            "4      0  Nah I don't think he goes to usf, he lives aro...\n",
            "...   ..                                                ...\n",
            "5567   1  This is the 2nd time we have tried 2 contact u...\n",
            "5568   0              Will Ì_ b going to esplanade fr home?\n",
            "5569   0  Pity, * was in mood for that. So...any other s...\n",
            "5570   0  The guy did some bitching but I acted like i'd...\n",
            "5571   0                         Rofl. Its true to its name\n",
            "\n",
            "[5572 rows x 2 columns]\n",
            "      v1                                                 v2\n",
            "0      0      Anything lar then Ì_ not going home 4 dinner?\n",
            "1      0    Yo, any way we could pick something up tonight?\n",
            "2      0                             Yes.i'm in office da:)\n",
            "3      0  If i let you do this, i want you in the house ...\n",
            "4      0                            Nah, I'm a perpetual DD\n",
            "...   ..                                                ...\n",
            "5567   0  U wake up already? Wat u doing? U picking us u...\n",
            "5568   0                          Neshanth..tel me who r u?\n",
            "5569   0                                            Havent.\n",
            "5570   0                              Hui xin is in da lib.\n",
            "5571   0          HI ITS KATE CAN U GIVE ME A RING ASAP XXX\n",
            "\n",
            "[5572 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QGyHG4lBISP2"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In the lab we use built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
        "**In this HW, you are required to implement the `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
        "\n",
        "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
        "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
        "\n",
        "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "793EFaQYhHeR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "17e801b2-e167-4920-d69d-8947c6571d5d"
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def preprocess_data(data):\n",
        "    # This function should return a list of lists of preprocessed tokens for each message\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    \n",
        "    preprocessed_data = None\n",
        "    preprocessed_data = []\n",
        "    for message in data:\n",
        "      preprocessed_data.append(word_tokenize(message))\n",
        "    print(type(preprocessed_data))\n",
        "    return preprocessed_data\n",
        "    \n",
        "\n",
        "train_data = preprocess_data(train_texts)\n",
        "val_data = preprocess_data(val_texts)\n",
        "test_data = preprocess_data(test_texts)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<class 'list'>\n",
            "<class 'list'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM2qpOKpjVbD",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
        "        # Create a token indexer, self.token_to_index, that will return index of the token in self.vocab\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        vocab_dict = {}\n",
        "\n",
        "        for elem in dataset:\n",
        "          for item in elem: \n",
        "            if item in vocab_dict.keys():\n",
        "              vocab_dict[item] += 1\n",
        "            else:\n",
        "              vocab_dict[item] = 1\n",
        "        most = sorted(vocab_dict, key = vocab_dict.get, reverse = True)\n",
        "        self.vocab_list = most[:self.max_features]\n",
        "\n",
        "        index = 0\n",
        "        self.token_to_index = {}\n",
        "        for elem in self.vocab_list:\n",
        "          self.token_to_index[elem] = index\n",
        "          index += 1\n",
        "        pass\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        # This function transforms text dataset into a matrix, data_matrix\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        row = 0\n",
        "        for mylist in dataset: #get a particular list of words in the dataset:\n",
        "          for word in self.vocab_list:\n",
        "            if word in mylist:\n",
        "              data_matrix[row][self.token_to_index[word]] = mylist.count(word)\n",
        "          row += 1\n",
        "        return data_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wXMrZXlZjcH7",
        "colab": {}
      },
      "source": [
        "max_features = 3000 # TODO: Replace None with a number\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGLg6udky1zo"
      },
      "source": [
        "You can add more features to the feature matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s80GgEm6F5DG",
        "outputId": "591d8e16-1b39-4bf6-db00-799470256f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYOUR CODE GOES HERE\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wtm7a6JWu9-3"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train logistic regression model and save prediction for train, val and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wq9stSAbAIZe",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Fit the model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction using the trained model\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j-Abw7JOqD_"
      },
      "source": [
        "## Performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Akg9LvP5DGE8"
      },
      "source": [
        "Your task is to report train, val, test accuracies and F1 scores.\n",
        "**You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.**\n",
        "\n",
        "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "chqVbKH6kZyY",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y_true, y_pred): \n",
        "    # Calculate accuracy of the model's prediction\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    accuracy = None\n",
        "    correct = 0\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == y_pred[i]:\n",
        "        correct += 1\n",
        "    accuracy = correct/len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "def f1_score(y_true, y_pred): \n",
        "    # Calculate F1 score of the model's prediction\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    f1 = None\n",
        "    tp = 0 \n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1:\n",
        "        if y_pred[i] == 1:\n",
        "          tp += 1\n",
        "    fp = 0\n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 0:\n",
        "        if y_pred[i] == 1:\n",
        "          fp += 1\n",
        "    fn = 0 \n",
        "    for i in range(len(y_true)):\n",
        "      if y_true[i] == 1:\n",
        "        if y_pred[i] == 0:\n",
        "          fn += 1\n",
        "    precision = tp / (tp+fp)\n",
        "    recall = tp / (tp+fn)\n",
        "    f1 = 2 * ((precision*recall) / (precision+recall))\n",
        "\n",
        "    return f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MqrMw0udDD04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "0a0193bb-3659-4fe1-c001-8f57ea458dfc"
      },
      "source": [
        "print(y_train)\n",
        "print(y_train_pred)\n",
        "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "Training accuracy: 0.994, F1 score: 0.979\n",
            "Validation accuracy: 0.981, F1 score: 0.926\n",
            "Test accuracy: 0.980, F1 score: 0.917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FW7P84giGgP4"
      },
      "source": [
        "**Question.**\n",
        "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
        "\n",
        "**Your answer:** In logistic regression for binary classification, we are trying to optimize the loss or error function. This function tells us how well our hypothesis matches the true labelling of the training point.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ak0h71krLPqX"
      },
      "source": [
        "**Question.**\n",
        "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
        "\n",
        "**Your answer:** Having a high accuracy does not imply that the model is great. This is because of the presence of false negatives. For example, suppose we were working with a dataset with 100 days, 85 of which it did not rain, and 15 of which it did rain. If our model was only predicting that it did not rain, the accuracy would be 85%, which is very high, but it would be predicting no rain for those 15 days. The F1 score alleviates this problem by taking into account the precision and recall which can therefore give a more appropriate idea of the model's validity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L_RDI0qdOxwM"
      },
      "source": [
        "### Exploration of predicitons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DHR2OqYCDOxs"
      },
      "source": [
        "Show a few examples with true+predicted labels on the train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5yv8GD-UGXvR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "outputId": "db934388-8046-4d44-b213-b5a5c96b7d2f"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "# 1 - spam, 0 - ham\n",
        "print(\"Training set examples: \")\n",
        "for i in range(2353,2360):\n",
        "  print(train_data[i])\n",
        "  print(\"y_train label:\", y_train[i])\n",
        "  print(\"y_train_pred prediction: \", y_train_pred[i])\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Validation set examples: \")\n",
        "for i in range(1,8):\n",
        "  print(val_data[i])\n",
        "  print(\"y_val label:\", y_val[i])\n",
        "  print(\"y_val_pred prediction: \", y_val_pred[i])\n",
        "\n",
        "# print(train_data[2353:2360])\n",
        "# print(y_train[2353:2360])\n",
        "# print(y_train_pred[2353:2360])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set examples: \n",
            "['Urgent', 'Ur', 'å£500', 'guaranteed', 'award', 'is', 'still', 'unclaimed', '!', 'Call', '09066368327', 'NOW', 'closingdate04/09/02', 'claimcode', 'M39M51', 'å£1.50pmmorefrommobile2Bremoved-MobyPOBox734LS27YF']\n",
            "y_train label: 1\n",
            "y_train_pred prediction:  1\n",
            "['Free', '1st', 'week', 'entry', '2', 'TEXTPOD', '4', 'a', 'chance', '2', 'win', '40GB', 'iPod', 'or', 'å£250', 'cash', 'every', 'wk', '.', 'Txt', 'POD', 'to', '84128', 'Ts', '&', 'Cs', 'www.textpod.net', 'custcare', '08712405020', '.']\n",
            "y_train label: 1\n",
            "y_train_pred prediction:  1\n",
            "['No', 'other', 'Valentines', 'huh', '?', 'The', 'proof', 'is', 'on', 'your', 'fb', 'page', '.', 'Ugh', 'I', \"'m\", 'so', 'glad', 'I', 'really', 'DID', \"N'T\", 'watch', 'your', 'rupaul', 'show', 'you', 'TOOL', '!']\n",
            "y_train label: 0\n",
            "y_train_pred prediction:  0\n",
            "['Aaooooright', 'are', 'you', 'at', 'work', '?']\n",
            "y_train label: 0\n",
            "y_train_pred prediction:  0\n",
            "['Well', 'done', 'ENGLAND', '!', 'Get', 'the', 'official', 'poly', 'ringtone', 'or', 'colour', 'flag', 'on', 'yer', 'mobile', '!', 'text', 'TONE', 'or', 'FLAG', 'to', '84199', 'NOW', '!', 'Opt-out', 'txt', 'ENG', 'STOP', '.', 'Box39822', 'W111WX', 'å£1.50']\n",
            "y_train label: 1\n",
            "y_train_pred prediction:  1\n",
            "['Customer', 'service', 'annoncement', '.', 'You', 'have', 'a', 'New', 'Years', 'delivery', 'waiting', 'for', 'you', '.', 'Please', 'call', '07046744435', 'now', 'to', 'arrange', 'delivery']\n",
            "y_train label: 1\n",
            "y_train_pred prediction:  1\n",
            "['Jus', 'chillaxin', ',', 'what', 'up']\n",
            "y_train label: 0\n",
            "y_train_pred prediction:  0\n",
            "\n",
            "\n",
            "Validation set examples: \n",
            "['Yo', ',', 'any', 'way', 'we', 'could', 'pick', 'something', 'up', 'tonight', '?']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  0\n",
            "['Yes.i', \"'m\", 'in', 'office', 'da', ':', ')']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  0\n",
            "['If', 'i', 'let', 'you', 'do', 'this', ',', 'i', 'want', 'you', 'in', 'the', 'house', 'by', '8am', '.']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  0\n",
            "['Nah', ',', 'I', \"'m\", 'a', 'perpetual', 'DD']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  0\n",
            "['FROM', '88066', 'LOST', 'å£12', 'HELP']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['Congrats', '!', 'Nokia', '3650', 'video', 'camera', 'phone', 'is', 'your', 'Call', '09066382422', 'Calls', 'cost', '150ppm', 'Ave', 'call', '3mins', 'vary', 'from', 'mobiles', '16+', 'Close', '300603', 'post', 'BCM4284', 'Ldn', 'WC1N3XX']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  1\n",
            "['Then', 'any', 'special', 'there', '?']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "neMQ4VR9GVL3"
      },
      "source": [
        "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
        "\n",
        "**Your answer:** In the examples shown below, the model incorrectly identified the texts as not spam (predicted 0) when the true labels were spam (labelled 1). Upon closer examination of the texts, they appear as if they could pass as not spam, because they are worded like text messages or personal emails. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ssK0jRxGY3u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "d787e725-0cf5-414f-e470-53bb5ee65bd0"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "count = 0\n",
        "for i in range(len(val_data)):\n",
        "  if count != 10:\n",
        "    if y_val[i] != y_val_pred[i]:\n",
        "      print(val_data[i])\n",
        "      print(\"y_val label:\", y_val[i])\n",
        "      print(\"y_val_pred prediction: \", y_val_pred[i])\n",
        "      count += 1\n",
        "    "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['FROM', '88066', 'LOST', 'å£12', 'HELP']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['100', 'dating', 'service', 'cal', ';', 'l', '09064012103', 'box334sk38ch']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['0A', '$', 'NETWORKS', 'allow', 'companies', 'to', 'bill', 'for', 'SMS', ',', 'so', 'they', 'are', 'responsible', 'for', 'their', '\\\\suppliers\\\\', \"''\"]\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['For', 'sale', '-', 'arsenal', 'dartboard', '.', 'Good', 'condition', 'but', 'no', 'doubles', 'or', 'trebles', '!']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['XCLUSIVE', '@', 'CLUBSAISAI', '2MOROW', '28/5', 'SOIREE', 'SPECIALE', 'ZOUK', 'WITH', 'NICHOLS', 'FROM', 'PARIS.FREE', 'ROSES', '2', 'ALL', 'LADIES', '!', '!', '!', 'info', ':', '07946746291/07880867867']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['Hi', 'this', 'is', 'Amy', ',', 'we', 'will', 'be', 'sending', 'you', 'a', 'free', 'phone', 'number', 'in', 'a', 'couple', 'of', 'days', ',', 'which', 'will', 'give', 'you', 'an', 'access', 'to', 'all', 'the', 'adult', 'parties', '...']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['Email', 'AlertFrom', ':', 'Jeri', 'StewartSize', ':', '2KBSubject', ':', 'Low-cost', 'prescripiton', 'drvgsTo', 'listen', 'to', 'email', 'call', '123']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['Back', '2', 'work', '2morro', 'half', 'term', 'over', '!', 'Can', 'U', 'C', 'me', '2nite', '4', 'some', 'sexy', 'passion', 'B4', 'I', 'have', '2', 'go', 'back', '?', 'Chat', 'NOW', '09099726481', 'Luv', 'DENA', 'Calls', 'å£1/minMobsmoreLKPOBOX177HP51FL']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n",
            "['I', '(', 'Career', 'Tel', ')', 'have', 'added', 'u', 'as', 'a', 'contact', 'on', 'INDYAROCKS.COM', 'to', 'send', 'FREE', 'SMS', '.', 'To', 'remove', 'from', 'phonebook', '-', 'sms', 'NO', 'to', '&', 'lt', ';', '#', '&', 'gt', ';']\n",
            "y_val label: 0\n",
            "y_val_pred prediction:  1\n",
            "['Would', 'you', 'like', 'to', 'see', 'my', 'XXX', 'pics', 'they', 'are', 'so', 'hot', 'they', 'were', 'nearly', 'banned', 'in', 'the', 'uk', '!']\n",
            "y_val label: 1\n",
            "y_val_pred prediction:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ja1hoUIFp_C2"
      },
      "source": [
        "## End of Part 1.\n"
      ]
    }
  ]
}